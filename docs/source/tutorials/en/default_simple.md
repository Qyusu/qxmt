# Simple case using only the default dataset and model

In this tutorial, you will experience the workflow of managing experiments in quantum machine learning using the default features of QXMT. QXMT provides commonly used datasets, their preprocessing methods, quantum machine learning models, and evaluation metrics as default functionalities. By utilizing these features, you can efficiently create a baseline in the initial stages of your experiments.

QXMT manages experiments based on the following directory structure:

- **data**: Stores the data used in the experiments.
- **config**: Contains YAML files that manage various experiments.
- **experiments**: Stores the information and results of experiments, which are incrementally saved by the program. Each folder is organized by the name of the Experiment, which will be explained later, and contains the actual execution results under individual "run" folders.

``` bash
<your_project>
├── data
├── configs
│   ├── config_1.yaml
│   ├──   ⋮
│   └── config_n.yaml
└── experiments
    ├── <your_experiment_1>
    │   ├── experiment.json
    │   ├── run_1
    │   │   └── model.pkl
    │   ├── run_2
    │   ├──   ⋮
    │   └── run_n
    │   ⋮
    └── <your_experiment_n>
```


## 1. Experiment Setup
First, import the packages installed via pip. Then, create an `Experiment` instance to manage the entire experiment and initialize it using the `init()` method. The `Experiment` class allows several member variables to be specified, but for now, we will configure just the basic three. Each has the following roles:

- **name**: Sets the name of the experiment. A directory `experiments/{name}` will be created in the Current Directory based on this name.
- **desc**: A free-text field for managing a brief description of the experiment. It is not used in processing, and other than being a string, there are no restrictions, so feel free to document any necessary information for reviewing the experiment later.
- **auto_gen_mode**: A flag to control whether to use the feature for automatically generating descriptions for each run using an LLM (default is `False`). If you wish to use this feature, you need to run `pip install qxmt[llm]` when installing the package, and set the environment variable `USE_LLM` to `True`.

``` python
import qxmt

exp = qxmt.Experiment(
    name="simple_tutorial",
    desc="A simple experiment to check the operation of the QXMT library",
    auto_gen_mode=False,
).init()
```

Up to this point, we have completed the preparations for managing experiments using QXMT.


## 2. Run Configuration
In QXMT, each experimental trial is managed as a unit called a "Run." There are two methods to configure a Run: by directly passing instances such as datasets and models to the `exp.run()` method, or by using a YAML config file. While the instance-based method is suitable for ad-hoc validations due to its ease of execution, for more formal experimental management in research, we recommend using the more feature-rich config-based method.

Here, we will explain the configuration items based on the config file used in this tutorial.

- **description**: Provides a description for the Run. The description mentioned in "1. Experiment Configuration" is for the entire experiment, while this description is specific to the Run. If you set this value to an empty string and `auto_gen_mode=True`, a description automatically generated by an LLM will be applied.
- **dataset**: Specifies the dataset to be used for the run. The `type` can be "file" or "generate." If "file" is specified, the dataset prepared by you will be used; if "generate" is specified, dummy data will be generated. In this tutorial, we will use the "generate" mode, but the "file" mode, which is closer to real-world applications, is introduced in another tutorial.
- **device**: Configures the quantum device for running simulations. Currently, the config-based settings support only pennylane.
- **feature_map**: If using a quantum feature map, specify the module name (`module_name`) and class name (`implement_name`) where the feature map is implemented. Required parameters can be specified in `params` as a dictionary. We will use the default feature maps provided.
- **kernel**: If using a kernel-based model, specify the module name (`module_name`) and class name (`implement_name`) where the kernel is implemented. Required parameters can be specified in `params` as a dictionary. We will use the default kernel provided.
- **model**: Specifies the machine learning model to be used, its parameters, and the filename for saving. We will use the default SVM model provided.
- **evaluation**: Lists the evaluation metrics to be used from the default set of metrics provided.

``` yaml
description: "Configuration file for the simple example"

dataset:
  type: "generate"
  path:
    data: "data/mnist_784/images.npy"
    label: "data/mnist_784/label.npy"
  random_seed: 42
  test_size: 0.2
  features: null
  raw_preprocess_logic: null
  transform_logic: null

device:
  platform: "pennylane"
  name: "default.qubit"
  n_qubits: 2
  shots: null

feature_map:
  module_name: "qxmt.feature_maps.pennylane"
  implement_name: "ZZFeatureMap"
  params:
    reps: 2

kernel:
  module_name: "qxmt.kernels.pennylane"
  implement_name: "FidelityKernel"
  params: {}

model:
  name: "qsvm"
  file_name: "model.pkl"
  params:
    C: 1.0
    gamma: 0.05

evaluation:
  default_metrics: ["accuracy", "precision", "recall", "f1_score"]

```


## 3. Running the Run
In this step, you will execute the Run configured in Step 2. There are two methods to execute a Run: by passing the config as a file path or by passing it as an instance. First, we will demonstrate how to execute it by specifying the path.

``` python
config_path = "../data/configs/simple.yaml"

# input config file
artifact_1, result_1 = exp.run(config_source=config_path)
```

When executing a Run, you can specify various additional arguments, but in this case, we are introducing the simplest method by specifying only the config. When a Run is executed, it returns `artifact` and `result`. The `artifact` includes the dataset (`artifact.dataset`) and machine learning model (`artifact.model`) used in the Run. The result contains the Run settings and model evaluation results. These can be used for visualization, as introduced in the next chapter, or for analysis during subsequent model development.

Next, we will demonstrate how to execute a Run by passing the config as an instance. This method is useful for making on-the-fly adjustments to model parameters and is beneficial when exploring the structure of a model.


``` python
import yaml

# load default config
update_config = yaml.safe_load(open(config))
# update model paramter
update_config["model"]["params"] = {'C': 0.1, 'gamma': 0.1}

# input the updated config instance
artifact_2, result_2 = exp.run(config_source=config)
```

The `artifact` and `result` obtained as the execution results will be in the same format as those obtained when running from a config file.

The results of each Run can be retrieved as a DataFrame of the metrics specified in `evaluation.default_metrics` by executing the following code:

``` python
# extract the results as a pandas dataframe
metrics_df = exp.runs_to_dataframe()

metrics_df.head()
# output
#       run_id  accuracy  precision  recall  f1_score
# 0          1	    0.75       0.66    0.75      0.70
# 1          2	    0.55       0.54    0.60      0.57
```


## 4. Visualization of Results
Several methods for visualizing the results of multiple Runs managed by Experiment are introduced. In addition to the methods introduced here, the library provides various other visualization techniques. Please refer to the API Reference and utilize those that suit your objectives. Additionally, the results of a Run, stored in the `result` from the previous chapter, can also be extracted as a DataFrame, allowing you to perform various analyses and visualizations on your own.


### 4.1 Visualizing the Label Distribution of the Dataset
In this section, we visualize the label distribution of the dummy dataset created. This visualization allows you to visually check if there is a significant deviation in distribution between the training and evaluation datasets. In the following example, only the dataset and the save path (`save_path`) are specified as arguments, but other options such as the graph title can also be passed as `kwargs`. Additionally, if `save_path=None` is specified, the graph will not be saved.

``` python
from qxmt.visualization import plot_2d_dataset

# extract dataset instance from run artifact
dataset = artifact_1.dataset

plot_2d_dataset(
  dataset=dataset,
  save_path=exp.experiment_dirc / f"run_{exp.current_run_id}/dataset.png"
  )
```

<img src="../../_static/images/tutorials/simple/dataset.png" alt="データセットの分布" title="データセットの分布">


### 4.2 Comparison of Evaluation Metrics
Here, we introduce how to visualize and compare evaluation metrics when multiple Runs are executed by changing parameters or the model structure. Using `plot_metrics_side_by_side`, you can create bar graphs with each Run on the X-axis by passing a list of the evaluation metric names you want to compare (column names of the DataFrame). Additionally, although not set in this example, if you want to narrow down the comparison to specific Runs, you can do so by setting a list of target run_ids, such as `run_ids=[1, 3, 5, 6]`, to the argument.

``` python
from qxmt.visualization import plot_metrics_side_by_side

# get run result as dataframe
df = exp.runs_to_dataframe()

plot_metrics_side_by_side(
  df=df,
  metrics=["accuracy", "recall", "precision", "f1_score"],
  run_ids=run_ids,
  save_path=exp.experiment_dirc / "side_by_side.png"
  )
```

<img src="../../_static/images/tutorials/simple/side_by_side.png" alt="評価指標の比較" title="評価指標の比較">


### 4.3 Decision Boundary of the Model
When using a Quantum Support Vector Machine (QSVM) as the model, you can visualize the decision boundary of the trained model. This visualization helps identify examples that are not classified correctly and provides insights into the characteristics of the model.

The model and dataset to be passed as arguments can be obtained from the Run's `artifact`. The `grid_resolution` parameter allows you to set the resolution of the visualized decision boundary—the higher the value, the smoother the boundary, and individual sample classification results can be examined. However, higher resolution significantly increases computation time, so adjust it according to your needs. The `support_vectors` argument takes a boolean value and controls whether the support vectors are highlighted with circles to improve visibility in the output graph.

``` python
from qxmt.visualization import plot_2d_decisionon_boundaries

# extract model and dataset instance from run artifact
model = artifact_1.model
dataset = artifact_1.dataset

plot_2d_decisionon_boundaries(
  model=model,
  dataet=dataset,
  grid_resolution=30,
  support_vectors=True,
  save_path=exp.experiment_dirc / f"run_{exp.current_run_id}/boundary.png")
```

<img src="../../_static/images/tutorials/simple/boundary.png" alt="モデルの決定境界" title="モデルの決定境界">
